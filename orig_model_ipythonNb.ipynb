{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "orig_model_ipythonNb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "uGluAhCL8wNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfeager\n",
        "from tensorflow.contrib.learn.python.learn import trainable\n",
        "import tensorflow.contrib.eager as tensor_eager\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.color import gray2rgb\n",
        "import cv2\n",
        "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\n",
        "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "OAmh0lhZ8wNK",
        "colab_type": "code",
        "outputId": "f71f4706-c48e-4f74-f1ec-1dce08a6215f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eager execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "63UqPhuh8wNN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jaYx5jfc8wNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_img_preprocess(image_path):\n",
        "    img_str = tf.read_file(image_path)\n",
        "\n",
        "    img_decode = tf.image.decode_jpeg(img_str, 3)\n",
        "\n",
        "    img = tf.cast(img_decode, tf.float32)\n",
        "\n",
        "    dim =512.0\n",
        "\n",
        "    height = tf.to_float(tf.shape(img)[1])\n",
        "\n",
        "    width = tf.to_float(tf.shape(img)[0])\n",
        "\n",
        "    scale = tf.cond(tf.greater(height, width), lambda: dim/width, lambda: dim/height)\n",
        "\n",
        "    newHeight = tf.to_int32(height * scale)\n",
        "    newWidth = tf.to_int32(width * scale)\n",
        "\n",
        "    img = tf.image.resize_images(img, [newHeight, newWidth])\n",
        "\n",
        "    \"\"\"VGG_MEAN = [123.68, 116.78, 103.94]  # This is R-G-B for Imagenet\n",
        "\n",
        "    img = tf.random_crop(img, [224, 224, 3])\n",
        "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
        "    img = img - means\n",
        "    \"\"\"\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
        "\n",
        "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
        "    img = img - means\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtrW671H8wNR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qLcxA69K8wNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRAWAMTu8wNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def deprocess_img(processed_img):\n",
        "  x = processed_img\n",
        "  # perform the inverse of the preprocessiing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1]\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x\n",
        "\n",
        "\n",
        "def restore_image(processed_image):\n",
        "    x = processed_image\n",
        "    x = np.squeeze(x, 0)\n",
        "\n",
        "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
        "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
        "    x = x + means\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrUaj9lw8wNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_style_loss(base_style, gram_target):\n",
        "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
        "  # height, width, num filters of each layer\n",
        "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
        "  height, width, channels = base_style.get_shape().as_list()\n",
        "  gram_style = gram_matrix(base_style)\n",
        "  \n",
        "  return tf.reduce_mean(tf.square(gram_style - gram_target))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KTUgipU8wNa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gram_matrix(input_tensor):\n",
        "\n",
        "    # We make the image channels first\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fV1ehn8I8wNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_style_loss(base_style, gram_target):\n",
        "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
        "  # height, width, num filters of each layer\n",
        "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
        "  height, width, channels = base_style.get_shape().as_list()\n",
        "  gram_style = gram_matrix(base_style)\n",
        "  \n",
        "  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DeRe9rfI8wNh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run(content_path, style_path, itrNum, content_weight, style_weight):\n",
        "\n",
        "    # get the images\n",
        "    # tf.enable_eager_execution()\n",
        "\n",
        "    content_image = load_img_preprocess(content_path)\n",
        "\n",
        "    style_image = load_img_preprocess(style_path)\n",
        "\n",
        "    \"\"\"\n",
        "    get the model from keras basically lets us extract the layers\n",
        "    and their corresponding intermediate and batch outputs\n",
        "\n",
        "    can do interesting things with the intermediate layers results\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    pretrained_net = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "    pretrained_net.trainable = False\n",
        "\n",
        "    style_layers = []\n",
        "\n",
        "    for i in range(1, 5):\n",
        "        style_layers.append('block{}_conv1'.format(i))\n",
        "\n",
        "    content_out_layers = [pretrained_net.get_layer('block5_conv2').output]\n",
        "\n",
        "    style_out_layers = []\n",
        "\n",
        "    for layer_name in style_layers:\n",
        "        style_out_layers.append(pretrained_net.get_layer(layer_name).output)\n",
        "\n",
        "    model_out = content_out_layers + style_out_layers\n",
        "\n",
        "    main_model = models.Model(pretrained_net.input, model_out)\n",
        "\n",
        "    for layer in main_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    style_features = [style_layer[0]\n",
        "                      for style_layer in main_model(style_image)[:5]]\n",
        "    content_features = [content_layer[0]\n",
        "                        for content_layer in main_model(content_image)[5:]]\n",
        "\n",
        "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "    # Set initial image\n",
        "    init_image = load_img_preprocess(content_path)\n",
        "    init_image = tensor_eager.Variable(init_image, dtype=tf.float32)\n",
        "    # Create our optimizer\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
        "\n",
        "    loss_best = float('inf')\n",
        "    best_img = None\n",
        "\n",
        "    # clip the input image \n",
        "    # using max dim and min dim\n",
        "    means = np.array([103.939, 116.779, 123.68])\n",
        "    min_dim = -means\n",
        "    max_dim = 255-means\n",
        "\n",
        "    for i in range(itrNum):\n",
        "        # compute loss for all layers\n",
        "        # compute style loss \n",
        "        # compute content loss\n",
        "        # compute their sum and produce gradients over the total loss\n",
        "        # optimize using the total loss and the input image\n",
        "        with tf.GradientTape() as gradi:\n",
        "            out_final_entireImage = main_model(init_image)\n",
        "            style_out = out_final_entireImage[:5]\n",
        "            content_out = out_final_entireImage[5:]\n",
        "            stylePoints = 0\n",
        "            contentPoints = 0\n",
        "\n",
        "            # equal weight across the contributions of all layers\n",
        "            content_layer_norm = 1/float(5)\n",
        "            for out, inter in zip(content_features, content_out):\n",
        "                contentPoints= contentPoints + content_layer_norm*(tf.reduce_mean(tf.square(inter[0] - out)))\n",
        "\n",
        "            \n",
        "            style_layer_norm = 1/float(5)\n",
        "            for out, inter in zip(gram_style_features, style_out):\n",
        "                stylePoints = stylePoints + style_layer_norm*get_style_loss(inter[0],out)\n",
        "\n",
        "            contentPoints = contentPoints*contentPoints\n",
        "            stylePoints = stylePoints*stylePoints\n",
        "\n",
        "            totalLoss = contentPoints + stylePoints\n",
        "\n",
        "        gradients = gradi.gradient(totalLoss, init_image)\n",
        "        opt.apply_gradients([(gradients,init_image)])\n",
        "\n",
        "        clip_initImage = tf.clip_by_value(init_image, min_dim, max_dim)\n",
        "\n",
        "        init_image.assign(clip_initImage)\n",
        "        \n",
        "        preserve_colors = True\n",
        "        \n",
        "        if(totalLoss<loss_best):\n",
        "            loss_best = totalLoss\n",
        "            best_img = deprocess_img(init_image.numpy())\n",
        "    if(preserve_colors == True):\n",
        "        orig_img = np.clip(content_image, 0, 255)\n",
        "        out_img = np.clip(best_img, 0, 255)\n",
        "\n",
        "        out_grayscale_img = rgb2gray(out_img)\n",
        "        out_grayscale_rgb_img = gray2rgb(out_grayscale_img)\n",
        "\n",
        "        #out_grayscale_yuv = np.array(Image.fromarray(out_grayscale_rgb_img.astype(np.uint8)).convert('YCbCr'))\n",
        "        #orig_yuv = np.array(Image.fromarray(orig_img.astype(np.uint8)).convert('YCbCr'))\n",
        "\n",
        "        out_grayscale_yuv = cv2.cvtColor(Image.fromarray(out_grayscale_rgb_img.astype(np.uint8)), cv2.COLOR_BGR2YUV)\n",
        "        orig_yuv = cv2.cvtColor(Image.fromarray(orig_img.astype(np.uint8)), cv2.COLOR_BGR2YUV)\n",
        "        \n",
        "        x,y,z = orig_img.shape\n",
        "        rgb_yuv = np.empty((x,y,3), dtype=np.uint8)\n",
        "        rgb_yuv[...,0] = out_grayscale_yuv[...,0]\n",
        "        rgb_yuv[...,1] = orig_yuv[...,1]\n",
        "        rgb_yuv[...,2] = orig_yuv[...,2]\n",
        "\n",
        "        best_img = np.array(Image.fromarray(combined_yuv, 'YCbCr').convert('RGB'))\n",
        "        \n",
        "    plt.figure(figsize=(10,10))\n",
        "    display_img = np.squeeze(best_img, axis=0)\n",
        "    plt.imshow(display_img)\n",
        "    plt.title('output image')\n",
        "    plt.show()\n",
        "    return best_img, loss_best\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sf41fN0v8wNk",
        "colab_type": "code",
        "outputId": "b28d5635-19b2-452e-8501-20cf09d32808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "best_starry_night, best_loss = run('/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg','/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg', 10,1e3,1e-2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mtypekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'typestr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 533, 3), '|u1')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-346e0fe5ee02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_starry_night\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-debeb43cc237>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(content_path, style_path, itrNum, content_weight, style_weight)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m#orig_yuv = np.array(Image.fromarray(orig_img.astype(np.uint8)).convert('YCbCr'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout_grayscale_yuv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grayscale_rgb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2YUV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0morig_yuv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2YUV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m             \u001b[0;31m# print(typekey)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9G_oFTHz8wNn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJteUV958wNo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pf3sqFKe8wNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnIuKVaV8wNs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}