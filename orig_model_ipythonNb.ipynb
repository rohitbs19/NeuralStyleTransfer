{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfeager\n",
    "from tensorflow.contrib.learn.python.learn import trainable\n",
    "import tensorflow.contrib.eager as tensor_eager\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_preprocess(image_path):\n",
    "    img_str = tf.read_file(image_path)\n",
    "\n",
    "    img_decode = tf.image.decode_jpeg(img_str, 3)\n",
    "\n",
    "    img = tf.cast(img_decode, tf.float32)\n",
    "\n",
    "    dim =512.0\n",
    "\n",
    "    height = tf.to_float(tf.shape(img)[1])\n",
    "\n",
    "    width = tf.to_float(tf.shape(img)[0])\n",
    "\n",
    "    scale = tf.cond(tf.greater(height, width), lambda: dim/width, lambda: dim/height)\n",
    "\n",
    "    newHeight = tf.to_int32(height * scale)\n",
    "    newWidth = tf.to_int32(width * scale)\n",
    "\n",
    "    img = tf.image.resize_images(img, [newHeight, newWidth])\n",
    "\n",
    "    \"\"\"VGG_MEAN = [123.68, 116.78, 103.94]  # This is R-G-B for Imagenet\n",
    "\n",
    "    img = tf.random_crop(img, [224, 224, 3])\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    img = img - means\n",
    "    \"\"\"\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    img = img - means\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deprocess_img(processed_img):\n",
    "  x = processed_img\n",
    "  # perform the inverse of the preprocessiing step\n",
    "  x[:, :, 0] += 103.939\n",
    "  x[:, :, 1] += 116.779\n",
    "  x[:, :, 2] += 123.68\n",
    "  x = x[:, :, ::-1]\n",
    "\n",
    "  x = np.clip(x, 0, 255).astype('uint8')\n",
    "  return x\n",
    "\n",
    "\n",
    "def restore_image(processed_image):\n",
    "    x = processed_image\n",
    "    x = np.squeeze(x, 0)\n",
    "\n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    x = x + means\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  \n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "\n",
    "    # We make the image channels first\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    return gram / tf.cast(n, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  \n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(content_path, style_path, itrNum, content_weight, style_weight):\n",
    "\n",
    "    # get the images\n",
    "    # tf.enable_eager_execution()\n",
    "\n",
    "    content_image = load_img_preprocess(content_path)\n",
    "\n",
    "    style_image = load_img_preprocess(style_path)\n",
    "\n",
    "    \"\"\"\n",
    "    get the model from keras basically lets us extract the layers\n",
    "    and their corresponding intermediate and batch outputs\n",
    "\n",
    "    can do interesting things with the intermediate layers results\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pretrained_net = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "    pretrained_net.trainable = False\n",
    "\n",
    "    style_layers = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        style_layers.append('block{}_conv1'.format(i))\n",
    "\n",
    "    content_out_layers = [pretrained_net.get_layer('block5_conv2').output]\n",
    "\n",
    "    style_out_layers = []\n",
    "\n",
    "    for layer_name in style_layers:\n",
    "        style_out_layers.append(pretrained_net.get_layer(layer_name).output)\n",
    "\n",
    "    model_out = content_out_layers + style_out_layers\n",
    "\n",
    "    main_model = models.Model(pretrained_net.input, model_out)\n",
    "\n",
    "    for layer in main_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    style_features = [style_layer[0]\n",
    "                      for style_layer in main_model(style_image)[:5]]\n",
    "    content_features = [content_layer[0]\n",
    "                        for content_layer in main_model(content_image)[5:]]\n",
    "\n",
    "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "\n",
    "    # Set initial image\n",
    "    init_image = load_img_preprocess(content_path)\n",
    "    init_image = tensor_eager.Variable(init_image, dtype=tf.float32)\n",
    "    # Create our optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "\n",
    "    loss_best = float('inf')\n",
    "    best_img = None\n",
    "\n",
    "    # clip the input image \n",
    "    # using max dim and min dim\n",
    "    means = np.array([103.939, 116.779, 123.68])\n",
    "    min_dim = -means\n",
    "    max_dim = 255-means\n",
    "\n",
    "    for i in range(itrNum):\n",
    "        # compute loss for all layers\n",
    "        # compute style loss \n",
    "        # compute content loss\n",
    "        # compute their sum and produce gradients over the total loss\n",
    "        # optimize using the total loss and the input image\n",
    "        with tf.GradientTape() as gradi:\n",
    "            out_final_entireImage = main_model(init_image)\n",
    "            style_out = out_final_entireImage[:5]\n",
    "            content_out = out_final_entireImage[5:]\n",
    "            stylePoints = 0\n",
    "            contentPoints = 0\n",
    "\n",
    "            # equal weight across the contributions of all layers\n",
    "            content_layer_norm = 1/float(5)\n",
    "            for out, inter in zip(content_features, content_out):\n",
    "                contentPoints= contentPoints + content_layer_norm*(tf.reduce_mean(tf.square(inter[0] - out)))\n",
    "\n",
    "            \n",
    "            style_layer_norm = 1/float(5)\n",
    "            for out, inter in zip(gram_style_features, style_out):\n",
    "                stylePoints = stylePoints + style_layer_norm*get_style_loss(inter[0],out)\n",
    "\n",
    "            contentPoints = contentPoints*contentPoints\n",
    "            stylePoints = stylePoints*stylePoints\n",
    "\n",
    "            totalLoss = contentPoints + stylePoints\n",
    "\n",
    "        gradients = gradi.gradient(totalLoss, init_image)\n",
    "        opt.apply_gradients([(gradients,init_image)])\n",
    "\n",
    "        clip_initImage = tf.clip_by_value(init_image, min_dim, max_dim)\n",
    "\n",
    "        init_image.assign(clip_initImage)\n",
    "\n",
    "        if(totalLoss<loss_best):\n",
    "            loss_best = totalLoss\n",
    "            best = deprocess_img(init_image.numpy())\n",
    "\n",
    "    return best_img, loss_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "clip_value_min and clip_value_max must be either of the same shape as input, or a scalar. input shape: [1,682,512,3]clip_value_min shape: [3]clip_value_max shape: [3] [Op:ClipByValue] name: clip_by_value/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-07c49209a6f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_starry_night\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src_images/Tuebingen_Neckarfront.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'src_images/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-01ba8776c701>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(content_path, style_path, itrNum, content_weight, style_weight)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mclip_initImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0minit_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_initImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rao_s\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[1;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[0;32m     64\u001b[0m                                       \u001b[0mclip_value_min\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                       \u001b[0mclip_value_max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                                       name=name)\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ClipByValue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rao_s\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[1;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[0;32m   1759\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m       return clip_by_value_eager_fallback(\n\u001b[1;32m-> 1761\u001b[1;33m           t, clip_value_min, clip_value_max, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1763\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rao_s\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mclip_by_value_eager_fallback\u001b[1;34m(t, clip_value_min, clip_value_max, name, ctx)\u001b[0m\n\u001b[0;32m   1778\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m   _result = _execute.execute(b\"ClipByValue\", 1, inputs=_inputs_flat,\n\u001b[1;32m-> 1780\u001b[1;33m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m   1781\u001b[0m   _execute.record_gradient(\n\u001b[0;32m   1782\u001b[0m       \"ClipByValue\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mc:\\users\\rao_s\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rao_s\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: clip_value_min and clip_value_max must be either of the same shape as input, or a scalar. input shape: [1,682,512,3]clip_value_min shape: [3]clip_value_max shape: [3] [Op:ClipByValue] name: clip_by_value/"
     ]
    }
   ],
   "source": [
    "\n",
    "best_starry_night, best_loss = run('src_images/Tuebingen_Neckarfront.jpg','src_images/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg', 10,1e3,1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
