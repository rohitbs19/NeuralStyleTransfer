{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfeager\n",
    "from tensorflow.contrib.learn.python.learn import trainable\n",
    "import tensorflow.contrib.eager as tensor_eager\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_preprocess(image_path):\n",
    "    img_str = tf.read_file(image_path)\n",
    "\n",
    "    img_decode = tf.image.decode_jpeg(img_str, 3)\n",
    "\n",
    "    img = tf.cast(img_decode, tf.float32)\n",
    "\n",
    "    dim =512.0\n",
    "\n",
    "    height = tf.to_float(tf.shape(img)[1])\n",
    "\n",
    "    width = tf.to_float(tf.shape(img)[0])\n",
    "\n",
    "    scale = tf.cond(tf.greater(height, width), lambda: dim/width, lambda: dim/height)\n",
    "\n",
    "    newHeight = tf.to_int32(height * scale)\n",
    "    newWidth = tf.to_int32(width * scale)\n",
    "\n",
    "    img = tf.image.resize_images(img, [newHeight, newWidth])\n",
    "\n",
    "    \"\"\"VGG_MEAN = [123.68, 116.78, 103.94]  # This is R-G-B for Imagenet\n",
    "\n",
    "    img = tf.random_crop(img, [224, 224, 3])\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    img = img - means\n",
    "    \"\"\"\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    img = img - means\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deprocess_img(processed_img):\n",
    "  x = processed_img\n",
    "  # perform the inverse of the preprocessiing step\n",
    "  x[:, :, 0] += 103.939\n",
    "  x[:, :, 1] += 116.779\n",
    "  x[:, :, 2] += 123.68\n",
    "  x = x[:, :, ::-1]\n",
    "\n",
    "  x = np.clip(x, 0, 255).astype('uint8')\n",
    "  return x\n",
    "\n",
    "\n",
    "def restore_image(processed_image):\n",
    "    x = processed_image\n",
    "    x = np.squeeze(x, 0)\n",
    "\n",
    "    VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "    means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "    x = x + means\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  \n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "\n",
    "    # We make the image channels first\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    return gram / tf.cast(n, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  \n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(content_path, style_path, itrNum, content_weight, style_weight):\n",
    "\n",
    "    # get the images\n",
    "    # tf.enable_eager_execution()\n",
    "\n",
    "    content_image = load_img_preprocess(content_path)\n",
    "\n",
    "    style_image = load_img_preprocess(style_path)\n",
    "\n",
    "    \"\"\"\n",
    "    get the model from keras basically lets us extract the layers\n",
    "    and their corresponding intermediate and batch outputs\n",
    "\n",
    "    can do interesting things with the intermediate layers results\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pretrained_net = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "    pretrained_net.trainable = False\n",
    "\n",
    "    style_layers = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        style_layers.append('block{}_conv1'.format(i))\n",
    "\n",
    "    content_out_layers = [pretrained_net.get_layer('block5_conv2').output]\n",
    "\n",
    "    style_out_layers = []\n",
    "\n",
    "    for layer_name in style_layers:\n",
    "        style_out_layers.append(pretrained_net.get_layer(layer_name).output)\n",
    "\n",
    "    model_out = content_out_layers + style_out_layers\n",
    "\n",
    "    main_model = models.Model(pretrained_net.input, model_out)\n",
    "\n",
    "    for layer in main_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    style_features = [style_layer[0]\n",
    "                      for style_layer in main_model(style_image)[:5]]\n",
    "    content_features = [content_layer[0]\n",
    "                        for content_layer in main_model(content_image)[5:]]\n",
    "\n",
    "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "\n",
    "    # Set initial image\n",
    "    init_image = load_img_preprocess(content_path)\n",
    "    init_image = tensor_eager.Variable(init_image, dtype=tf.float32)\n",
    "    # Create our optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "\n",
    "    loss_best = float('inf')\n",
    "    best_img = None\n",
    "\n",
    "    # clip the input image \n",
    "    # using max dim and min dim\n",
    "    max_dim = -np.array([103.939, 116.779, 123.68])\n",
    "    min_dim = 255-np.array([103.939, 116.779, 123.68]) \n",
    "\n",
    "    for i in range(itrNum):\n",
    "        # compute loss for all layers\n",
    "        # compute style loss \n",
    "        # compute content loss\n",
    "        # compute their sum and produce gradients over the total loss\n",
    "        # optimize using the total loss and the input image\n",
    "        with tf.GradientTape() as gradi:\n",
    "            out_final_entireImage = main_model(init_image)\n",
    "            style_out = out_final_entireImage[:5]\n",
    "            content_out = out_final_entireImage[5:]\n",
    "            stylePoints = 0\n",
    "            contentPoints = 0\n",
    "\n",
    "            # equal weight across the contributions of all layers\n",
    "            content_layer_norm = 1/float(5)\n",
    "            for out, inter in zip(content_features, content_out):\n",
    "                contentPoints= contentPoints + content_layer_norm*(tf.reduce_mean(tf.square(inter[0] - out)))\n",
    "\n",
    "            \n",
    "            style_layer_norm = 1/float(5)\n",
    "            for out, inter in zip(gram_style_features, style_out):\n",
    "                stylePoints = stylePoints + style_layer_norm*get_style_loss(inter[0],out)\n",
    "\n",
    "            contentPoints = contentPoints*contentPoints\n",
    "            stylePoints = stylePoints*stylePoints\n",
    "\n",
    "            totalLoss = contentPoints + stylePoints\n",
    "\n",
    "            gradients = gradi.gradient(totalLoss, init_image)\n",
    "        opt.apply_gradients([(gradients,init_image)])\n",
    "\n",
    "        clip_initImage = tf.clip_by_value(init_image, min_dim, max_dim)\n",
    "\n",
    "        init_image.assign(clip_initImage)\n",
    "\n",
    "        if(totalLoss<loss_best):\n",
    "            loss_best = totalLoss\n",
    "            best = init_image.numpy;\n",
    "            Image.fromarray(best)\n",
    "\n",
    "    return best_img, loss_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.enable_eager_execution()\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-07c49209a6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_starry_night\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src_images/Tuebingen_Neckarfront.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'src_images/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2612350577d7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(content_path, style_path, itrNum, content_weight, style_weight)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "best_starry_night, best_loss = run('src_images/Tuebingen_Neckarfront.jpg','src_images/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg', 10,1e3,1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
